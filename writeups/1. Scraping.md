Hello, this is the start of my journey. It is real hard trying to work on a research paper and also having a full time job that often bleeds into afterhours. Regardless, anything can be done with enough perseverance.

Alrighty, let's tackle the first objective. Collecting raw code.

##### The first question is: is there a tool that can already gather this for me?
After a good 5 minutes of googling, I could not find one. This doesn't seem like a particularly hard task so let me go ahead and write a scraper. Let's start with getting Java repositories.

I'll be using Python to write the scraper as it's just a great language for writing quick and dirty code that works.

Maybe an html scraper is not the best idea since GitHub offers a free api. Let me just figure out how to get that going. As per the example code in https://github.com/PyGithub/PyGithub, I managed to conjure up the following.

```python
from github import Github, Auth

auth = Auth.Token('xxx')
g = Github(auth=auth)

for repo in g.search_repositories('language:Java'):
	print(repo.clone_url)

g.close()
```

While this does manage to get only repos that are primarily Java, the default sorting method seems to bias highly starred repositories, which could lead to some bias in our data. To avoid that, let's sort by most recently updated since I want an idea of what the average modern programmer is doing.

```diff
- for repo in g.search_repositories('language:Java'):
+ for repo in g.search_repositories('language:Java', sort='updated'):
```

Good. Now I'm not sure how many repositories I exactly want. 5,000 seems like a arbitrary yet good number. So let's go ahead and get 5000 repository URLs per programming language and stick them in a file of their own.

Updated code w/ some logging: (btw this library makes this so easy. gotta love python)

```python
from github import Github, Auth

auth = Auth.Token('xxx')
g = Github(auth=auth)

N = 20

for lang in ('Java', 'Python', 'C++'):
	content = []
	for i, repo in enumerate(g.search_repositories('language:Java', sort='updated')[:N]):
		content += [repo.clone_url]
		if i % 100 == 0:
			print(f'{lang} repo {i+1}/{N}: {repo.clone_url}')

	f = open(f'repo_urls/{lang}.txt', 'w')
	f.write('\n'.join(content))
	f.close()
	
g.close()
```

First, let's modify our code and try it out with just 20.

![[Pasted image 20240826213803.png]]

Cool, now let's do all 5,000 (and hopefully not run into rate limit errors)
![[Pasted image 20240826214102.png]]

Surprise surprise. We ran into a rate limit. However, this library seems to be great at handling rate limits so let's just allow it to do its thing.

It seems that every minute we are allowed to get ~300 repos. That means each language will take about 17 minutes and the whole script will take nearly one hour. I'd say that's worth waiting out rather than attempting to find a way to shorten the time.

In the mean time, I'll start working the script to git clone all these rep-
![[Pasted image 20240826214556.png]]
Damn. It looks like pagination only lets us go up to 1000 results.

After some thought, I figured I could supplement the repositories with more results by using different sorting methods and search queries. However, I believe 1000 repositories should give us a pretty good sample size anyway, so let's stick with it for now.